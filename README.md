##  Log Processing using MapReduce 

 Name: Sahaja Peddaveerannagari

### Project Assignment 

The goal of the assignment is to process the log file generated by the [LogFileGenerator](https://github.com/0x1DOCD00D/CS441_Fall2022/tree/main/LogFileGenerator).<br>
The intention is to create jobs for each task given below: <br>

1. Compute a CSV file that shows the distribution of different types of messages across predefined time intervals and injected string instances of the designated regex pattern for these log message types. <br>
2. Compute time intervals sorted in the descending order that contained most log messages of the type ERROR with injected regex pattern string instances. <br>
3. Compute the number of generated log messages for each message type. <br>
4. Compute the number of characters in each log message for each log message type that contain the highest number of characters in the detected instances of the designated regex pattern. <br>

### MapReduce Architecture

MapReduce is a programming framework that allows us to perform distributed and parallel processing on large data sets in a distributed environment.<br>

#### MapReduce consists of two distinct tasks — Map and Reduce. <br>

#### Mapper 
- Mapper is a function which process the input data. The mapper processes the data and creates several small chunks of data. <br>
- It accepts a key-value pair (key, value) and produces a multiset of key-value pairs, {(k_1,v_1),…,(k_p,v_p)}.

#### Reducer
- Reducer takes the output of the Mapper (intermediate key-value pair) process each of them to generate the output. The output of the reducer is the final output.<br>
- It accepts a pair (k, {value}), where a key is mapped to a multiset of some values, and it produces a map of key-set of values pair, with the same key(k,{v_1,…,v_s}) and a set of values that may be different from the ones in the input map.<br>

![img.png](Images/img.png)

### Installations
- hadoop 3.3.4
- Scala 3.2.0
- jdk-11.0.16
- sbt-1.7.1
- Intellij IDEA 2022.2.2 (Ultimate)

### Building the Homework
To build the Homework: <br>
Clone this repository through command line using
> git clone https://github.com/sahajareddyp/LogFileGenrator.git <br> 

Open IntelliJ IDEA and navigate File -> Open Folder -> LogFileGenerator <br>
To run tests, use the command
> sbt clean compile test

To build, run
> sbt clean compile assembly

in the command line or

> clean compile assembly

sequentially in sbt shell

The above command will create a jar file named LogFileGenerator_input.log inside the folder ./log/output , which means build process went through successfully.

### Running the Homework

To run the Homework: <br>
1. Go to cmd line, go to the hadoop directory, hadoop-3.3.4 and in that to sbin directory. <br>
2. Change the user from root to hdfs by running the command hdfs namenode -format. <br>
3. To start all the nodes, run the command- start-all.cmd which starts all the hadoop daemons all at once.

// Some steps to be added 

### AWS Elastic MapReduce Execution  
#### Setup
- Sign In to Amazon Web Services. <br>
 
#### AWS EC2 (Elastic Compute Cloud). <br>
- Navigate to AWS EC2. <br>
- On EC2 dashboard, on the left Panel search for Key-Pair. 
- Create a Key-Pair

#### AWS S3 (Simple Storage Service) <br>
- Navigate to AWS S3
- Create an S3 bucket
- Upload the jar and log files in the S3 bucket created.

#### AWS EMR (Elastic MapReduce) <br>
- Navigate to AWS EMR
  - Create an EMR cluster
   1. Enter a name. <br>
   2. Select the S3 bucket created earlier. <br>
   3. Select the Key-Pair created earlier
- Wait till the cluster is ready
- Navigate to Steps tab under AWS EMR
- Add steps for individual jobs
  1. Click on Add Step
  2. Select the jar from AWS S3 uploaded earlier
  
Provide arguments in format <br>

Execution <JobNumber> <s3BucketInputLocation> <s3BucketOutputLocation> Examples -

Job 1 <br>
Execution 1 s3://cs441-hw2-bucket/data/input s3://cs441-hw2-bucket/data/output
  
Job 2 <br>
Execution 2 s3://cs441-hw2-bucket/data/input s3://cs441-hw2-bucket/data/output

Job 3 <br>
Execution 3 s3://cs441-hw2-bucket/data/input s3://cs441-hw2-bucket/data/output

Job 4 <br>
Execution 4 s3://cs441-hw2-bucket/data/input s3://cs441-hw2-bucket/data/output

### YouTube Link

### Tasks

Task -1

![img_5.png](Images/img_5.png)


Task-2

![img_6.png](Images/img_6.png)


Task-2(Part-2)

![img_9.png](Images/img_9.png)




Task-3

![img_7.png](Images/img_7.png)

Task-4

![img_8.png](Images/img_8.png)







